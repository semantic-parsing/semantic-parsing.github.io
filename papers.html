<!DOCTYPE html>
<html lang="en-us">

  <head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-153224273-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-153224273-6');
  </script>

  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="keywords" content="semantic parsing, query languages, meaning representations, machine learning, discourse representations">

  <title>
    
      Search all Publications on Semantic Parsing &middot; Semantic Parsing
    
  </title>

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="shortcut icon" href="/public/favicon.svg">
  <link rel="search" href="/public/opensearchdescription.xml" 
      type="application/opensearchdescription+xml" 
      title="ML4Code" />

  <script src="https://code.jquery.com/jquery-3.2.1.min.js"
  integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
  crossorigin="anonymous"></script>
  
  <link rel="stylesheet" type="text/css" href="//cdn.datatables.net/1.10.16/css/jquery.dataTables.min.css">
  <script type="text/javascript" charset="utf8" src="//cdn.datatables.net/1.10.16/js/jquery.dataTables.min.js"></script>
</head>


  <body class="theme-base-07 layout-reverse">

    <a href='/contributing.html' class='ribbon'>Contribute to Semantic Parsing</a>
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Semantic Parsing
        </a>
      </h1>
      <p class="lead">Research on Modeling and Data resources for Semantic Parsing.</p>      
    </div>

  <nav class="sidebar-nav">
   <div class="sidebar-item"><p style="font-size: 12px">Search related work <input type='text' id='searchTarget' size="16"/> <button onClick="search();">Go</button></p></div>
   <a class="sidebar-nav-item active" href="/papers.html">List of Papers</a>
   <a class="sidebar-nav-item" href="/tags.html">Papers by Tag</a>
   <a class="sidebar-nav-item" href="/tsne-viz.html">2D Map of Papers</a>
   <a class="sidebar-nav-item" href="/topic-viz.html">Topic-based Explorer</a>

   <a class="sidebar-nav-item" href="/base-taxonomy/">Core Taxonomy</a>


  <a class="sidebar-nav-item" href="/resources.html">Resources, Courses &#38; Events</a>
  <a class="sidebar-nav-item" href="/contributing.html">Contributing</a>
  </nav>

  <div class="sidebar-item">
    <p style="font-size: 12px">Contact <a href="https://rajaswa.github.io/">Rajaswa Patil</a> about this survey or website.
    <span style="font-size: 9px">
      Made with <a href="https://jekyllrb.com">Jekyll</a> and <a href="https://github.com/poole/hyde">Hyde</a>.
    </span></p>
  </div>
</div></div>

<script>
$("#searchTarget").keydown(function (e) {	
  if (e.keyCode == 13) {
    search();
  }
});

function search() {
  try {
    ga('send', 'event', 'search', 'search', $("#searchTarget").val());
  } finally {
    window.location = "/papers.html#" + $("#searchTarget").val();
  }
}
</script>


    <div class="content container">
      Search across all paper titles, abstracts, authors by using the search field.
Please consider <a href="/contributing.html">contributing</a> by updating
the information of existing papers or adding new work.

<table id="allPapers">
<thead><th>Year</th><th>Title</th><th>Authors</th><th>Venue</th><th>Abstract</th></thead><tbody>



<tr>
	<td>2021</td>
	<td><a href="/publications/sherborne2021zero/">Zero-Shot Cross-lingual Semantic Parsing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Zero-Shot Cross-lingual Semantic Parsing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Zero-Shot Cross-lingual Semantic Parsing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Zero-Shot%20Cross-lingual%20Semantic%20Parsing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Tom Sherborne, Mirella Lapata</td>
	<td></td>
	<td><p>Recent work in crosslingual semantic parsing has successfully applied machine translation to localize accurate parsing to new languages. However, these advances assume access to high-quality machine translation systems, and tools such as word aligners, for all test languages. We remove these assumptions and study cross-lingual semantic parsing as a zero-shot problem without parallel data for 7 test languages (DE, ZH, FR, ES, PT, HI, TR). We propose a multi-task encoder-decoder model to transfer parsing knowledge to additional languages using only English-Logical form paired data and unlabeled, monolingual utterances in each test language. We train an encoder to generate language-agnostic representations jointly optimized for generating logical forms or utterance reconstruction and against language discriminability. Our system frames zero-shot parsing as a latent-space alignment problem and finds that pre-trained models can be improved to generate logical forms with minimal cross-lingual transfer penalty. Experimental results on Overnight and a new executable version of MultiATIS++ find that our zero-shot approach performs above back-translation baselines and, in some cases, approaches the supervised upper bound.</p>
</td>
	<td>sql cross-lingual multi-lingual transformer </td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/hazoom2021text/">Text-to-SQL in the Wild: A Naturally-Occurring Dataset Based on Stack Exchange Data</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Text-to-SQL in the Wild: A Naturally-Occurring Dataset Based on Stack Exchange Data' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Text-to-SQL in the Wild: A Naturally-Occurring Dataset Based on Stack Exchange Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Text-to-SQL%20in%20the%20Wild:%20A%20Naturally-Occurring%20Dataset%20Based%20on%20Stack%20Exchange%20Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Moshe Hazoom, Vibhor Malik, Ben Bogin</td>
	<td>NLP4Prog</td>
	<td><p>Most available semantic parsing datasets, comprising of pairs of natural utterances and logical forms, were collected solely for the purpose of training and evaluation of natural language understanding systems. As a result, they do not contain any of the richness and variety of natural-occurring utterances, where humans ask about data they need or are curious about. In this work, we release SEDE, a dataset with 12,023 pairs of utterances and SQL queries collected from real usage on the Stack Exchange website. We show that these pairs contain a variety of real-world challenges which were rarely reflected so far in any other semantic parsing dataset, propose an evaluation metric based on comparison of partial query clauses that is more suitable for real-world queries, and conduct experiments with strong baselines, showing a large gap between the performance on SEDE compared to other common datasets.</p>
</td>
	<td>sql evaluation dataset </td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/wang2021meta/">Meta-Learning for Domain Generalization in Semantic Parsing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Meta-Learning for Domain Generalization in Semantic Parsing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Meta-Learning for Domain Generalization in Semantic Parsing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Meta-Learning%20for%20Domain%20Generalization%20in%20Semantic%20Parsing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Bailin Wang, Mirella Lapata, Ivan Titov</td>
	<td>NAACL</td>
	<td><p>Recent work in crosslingual semantic parsing has successfully applied machine translation to localize accurate parsing to new languages. However, these advances assume access to high-quality machine translation systems, and tools such as word aligners, for all test languages. We remove these assumptions and study cross-lingual semantic parsing as a zero-shot problem without parallel data for 7 test languages (DE, ZH, FR, ES, PT, HI, TR). We propose a multi-task encoder-decoder model to transfer parsing knowledge to additional languages using only English-Logical form paired data and unlabeled, monolingual utterances in each test language. We train an encoder to generate language-agnostic representations jointly optimized for generating logical forms or utterance reconstruction and against language discriminability. Our system frames zero-shot parsing as a latent-space alignment problem and finds that pre-trained models can be improved to generate logical forms with minimal cross-lingual transfer penalty. Experimental results on Overnight and a new executable version of MultiATIS++ find that our zero-shot approach performs above back-translation baselines and, in some cases, approaches the supervised upper bound.</p>
</td>
	<td>sql meta-learning cross-domain transformer </td>
</tr>



<tr>
	<td>2020</td>
	<td><a href="/publications/lee2020pushing/">Pushing the Limits of AMR Parsing with Self-Learning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Pushing the Limits of AMR Parsing with Self-Learning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Pushing the Limits of AMR Parsing with Self-Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Pushing%20the%20Limits%20of%20AMR%20Parsing%20with%20Self-Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Young-Suk Lee, Ramon Fernandez Astudillo, Tahira Naseem, Revanth Gangi Reddy, Radu Florian, Salim Roukos</td>
	<td>EMNLP</td>
	<td><p>Abstract Meaning Representation (AMR) parsing has experienced a notable growth in performance in the last two years, due both to the impact of transfer learning and the development of novel architectures specific to AMR. At the same time, self-learning techniques have helped push the performance boundaries of other natural language processing applications, such as machine translation or question answering. In this paper, we explore different ways in which trained models can be applied to improve AMR parsing performance, including generation of synthetic text and AMR annotations as well as refinement of actions oracle. We show that, without any additional human annotations, these techniques improve an already performant parser and achieve state-of-the-art results on AMR 1.0 and AMR 2.0.</p>
</td>
	<td>amr transformer </td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/cai2020amr/">AMR Parsing via Graph-Sequence Iterative Inference</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=AMR Parsing via Graph-Sequence Iterative Inference' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=AMR Parsing via Graph-Sequence Iterative Inference' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=AMR%20Parsing%20via%20Graph-Sequence%20Iterative%20Inference' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Deng Cai, Wai Lam</td>
	<td>ACL</td>
	<td><p>We propose a new end-to-end model that treats AMR parsing as a series of dual decisions on the input sequence and the incrementally constructed graph. At each time step, our model performs multiple rounds of attention, reasoning, and composition that aim to answer two critical questions: (1) which part of the input sequence to abstract; and (2) where in the output graph to construct the new concept. We show that the answers to these two questions are mutually causalities. We design a model based on iterative inference that helps achieve better answers in both perspectives, leading to greatly improved parsing accuracy. Our experimental results significantly outperform all previously reported Smatch scores by large margins. Remarkably, without the help of any large-scale pre-trained language model (e.g., BERT), our model already surpasses previous state-of-the-art using BERT. With the help of BERT, we can push the state-of-the-art results to 80.2% on LDC2017T10 (AMR 2.0) and 75.4% on LDC2014T12 (AMR 1.0).</p>
</td>
	<td>amr transformer </td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/elgohary2020speak/">Speak to your Parser: Interactive Text-to-SQL with Natural Language Feedback</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Speak to your Parser: Interactive Text-to-SQL with Natural Language Feedback' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Speak to your Parser: Interactive Text-to-SQL with Natural Language Feedback' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Speak%20to%20your%20Parser:%20Interactive%20Text-to-SQL%20with%20Natural%20Language%20Feedback' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Ahmed Elgohary, Saghar Hosseini, Ahmed Hassan Awadallah</td>
	<td>ACL</td>
	<td><p>We study the task of semantic parse correction with natural language feedback. Given a natural language utterance, most semantic parsing systems pose the problem as one-shot translation where the utterance is mapped to a corresponding logical form. In this paper, we investigate a more interactive scenario where humans can further interact with the system by providing free-form natural language feedback to correct the system when it generates an inaccurate interpretation of an initial utterance. We focus on natural language to SQL systems and construct, SPLASH, a dataset of utterances, incorrect SQL interpretations and the corresponding natural language feedback. We compare various reference models for the correction task and show that incorporating such a rich form of feedback can significantly improve the overall semantic parsing accuracy while retaining the flexibility of natural language interaction. While we estimated human correction accuracy is 81.5%, our best model achieves only 25.1%, which leaves a large gap for improvement in future research. SPLASH is publicly available at this URL: https://github.com/MSR-LIT/Splash</p>
</td>
	<td>sql conversational dataset </td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/zhang2020did/">Did You Ask a Good Question? A Cross-Domain Question Intention Classification Benchmark for Text-to-SQL</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Did You Ask a Good Question? A Cross-Domain Question Intention Classification Benchmark for Text-to-SQL' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Did You Ask a Good Question? A Cross-Domain Question Intention Classification Benchmark for Text-to-SQL' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Did%20You%20Ask%20a%20Good%20Question?%20A%20Cross-Domain%20Question%20Intention%20Classification%20Benchmark%20for%20Text-to-SQL' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Yusen Zhang, Xiangyu Dong, Shuaichen Chang, Tao Yu, Peng Shi, Rui Zhang</td>
	<td></td>
	<td><p>Neural models have achieved significant results on the text-to-SQL task, in which most current work assumes all the input questions are legal and generates a SQL query for any input. However, in the real scenario, users can input any text that may not be able to be answered by a SQL query. In this work, we propose TriageSQL, the first cross-domain text-to-SQL question intention classification benchmark that requires models to distinguish four types of unanswerable questions from answerable questions. The baseline RoBERTa model achieves a 60% F1 score on the test set, demonstrating the need for further improvement on this task. Our dataset is available at this URL: https://github.com/chatc/TriageSQL</p>
</td>
	<td>sql cross-domain dataset </td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/nguyen2020a/">A Pilot Study of Text-to-SQL Semantic Parsing for Vietnamese</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Pilot Study of Text-to-SQL Semantic Parsing for Vietnamese' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Pilot Study of Text-to-SQL Semantic Parsing for Vietnamese' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=A%20Pilot%20Study%20of%20Text-to-SQL%20Semantic%20Parsing%20for%20Vietnamese' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Anh Tuan Nguyen, Mai Hoang Dao, Dat Quoc Nguyen</td>
	<td>EMNLP</td>
	<td><p>Semantic parsing is an important NLP task. However, Vietnamese is a low-resource language in this research area. In this paper, we present the first public large-scale Text-to-SQL semantic parsing dataset for Vietnamese. We extend and evaluate two strong semantic parsing baselines EditSQL (Zhang et al., 2019) and IRNet (Guo et al., 2019) on our dataset. We compare the two baselines with key configurations and find that: automatic Vietnamese word segmentation improves the parsing results of both baselines; the normalized pointwise mutual information (NPMI) score (Bouma, 2009) is useful for schema linking; latent syntactic features extracted from a neural dependency parser for Vietnamese also improve the results; and the monolingual language model PhoBERT for Vietnamese (Nguyen and Nguyen, 2020) helps produce higher performances than the recent best multilingual language model XLM-R (Conneau et al., 2020).</p>
</td>
	<td>sql cross-domain dataset </td>
</tr>



<tr>
	<td>2019</td>
	<td><a href="/publications/zhang2019amr/">AMR Parsing as Sequence-to-Graph Transduction</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=AMR Parsing as Sequence-to-Graph Transduction' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=AMR Parsing as Sequence-to-Graph Transduction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=AMR%20Parsing%20as%20Sequence-to-Graph%20Transduction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Sheng Zhang, Xutai Ma, Kevin Duh, Benjamin Van Durme</td>
	<td>ACL</td>
	<td><p>We propose an attention-based model that treats AMR parsing as sequence-to-graph transduction. Unlike most AMR parsers that rely on pre-trained aligners, external semantic resources, or data augmentation, our proposed parser is aligner-free, and it can be effectively trained with limited amounts of labeled AMR data. Our experimental results outperform all previously reported SMATCH scores, on both AMR 2.0 (76.3% on LDC2017T10) and AMR 1.0 (70.2% on LDC2014T12).</p>
</td>
	<td>amr transformer </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/yu2019sparc/">SParC: Cross-Domain Semantic Parsing in Context</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=SParC: Cross-Domain Semantic Parsing in Context' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=SParC: Cross-Domain Semantic Parsing in Context' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=SParC:%20Cross-Domain%20Semantic%20Parsing%20in%20Context' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Tao Yu, Rui Zhang, Michihiro Yasunaga, Yi Chern Tan, Xi Victoria Lin, Suyi Li, Heyang Er, Irene Li, Bo Pang, Tao Chen, Emily Ji, Shreya Dixit, David Proctor, Sungrok Shim, Jonathan Kraft, Vincent Zhang, Caiming Xiong, Richard Socher, Dragomir Radev</td>
	<td>ACL</td>
	<td><p>We present SParC, a dataset for cross-domain SemanticParsing in Context that consists of 4,298 coherent question sequences (12k+ individual questions annotated with SQL queries). It is obtained from controlled user interactions with 200 complex databases over 138 domains. We provide an in-depth analysis of SParC and show that it introduces new challenges compared to existing datasets. SParC demonstrates complex contextual dependencies, (2) has greater semantic diversity, and (3) requires generalization to unseen domains due to its cross-domain nature and the unseen databases at test time. We experiment with two state-of-the-art text-to-SQL models adapted to the context-dependent, cross-domain setup. The best model obtains an exact match accuracy of 20.2% over all questions and less than10% over all interaction sequences, indicating that the cross-domain setting and the con-textual phenomena of the dataset present significant challenges for future research. The dataset, baselines, and leaderboard are released at this URL: https://yale-lily.github.io/sparc</p>
</td>
	<td>sql cross-domain conversational dataset </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/yu2019cosql/">CoSQL: A Conversational Text-to-SQL Challenge Towards Cross-Domain Natural Language Interfaces to Databases</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=CoSQL: A Conversational Text-to-SQL Challenge Towards Cross-Domain Natural Language Interfaces to Databases' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=CoSQL: A Conversational Text-to-SQL Challenge Towards Cross-Domain Natural Language Interfaces to Databases' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=CoSQL:%20A%20Conversational%20Text-to-SQL%20Challenge%20Towards%20Cross-Domain%20Natural%20Language%20Interfaces%20to%20Databases' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Tao Yu, Rui Zhang, He Yang Er, Suyi Li, Eric Xue, Bo Pang, Xi Victoria Lin, Yi Chern Tan, Tianze Shi, Zihan Li, Youxuan Jiang, Michihiro Yasunaga, Sungrok Shim, Tao Chen, Alexander Fabbri, Zifan Li, Luyao Chen, Yuwen Zhang, Shreya Dixit, Vincent Zhang, Caiming Xiong, Richard Socher, Walter S Lasecki, Dragomir Radev</td>
	<td>EMNLP</td>
	<td><p>We present CoSQL, a corpus for building cross-domain, general-purpose database (DB) querying dialogue systems. It consists of 30k+ turns plus 10k+ annotated SQL queries, obtained from a Wizard-of-Oz (WOZ) collection of 3k dialogues querying 200 complex DBs spanning 138 domains. Each dialogue simulates a real-world DB query scenario with a crowd worker as a user exploring the DB and a SQL expert retrieving answers with SQL, clarifying ambiguous questions, or otherwise informing of unanswerable questions. When user questions are answerable by SQL, the expert describes the SQL and execution results to the user, hence maintaining a natural interaction flow. CoSQL introduces new challenges compared to existing task-oriented dialogue datasets:(1) the dialogue states are grounded in SQL, a domain-independent executable representation, instead of domain-specific slot-value pairs, and (2) because testing is done on unseen databases, success requires generalizing to new domains. CoSQL includes three tasks: SQL-grounded dialogue state tracking, response generation from query results, and user dialogue act prediction. We evaluate a set of strong baselines for each task and show that CoSQL presents significant challenges for future research. The dataset, baselines, and leaderboard will be released at this URL: https://yale-lily.github.io/cosql</p>
</td>
	<td>sql cross-domain conversational dataset </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/chen2019hybridqa/">HybridQA: A Dataset of Multi-Hop Question Answering over Tabular and Textual Data</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=HybridQA: A Dataset of Multi-Hop Question Answering over Tabular and Textual Data' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=HybridQA: A Dataset of Multi-Hop Question Answering over Tabular and Textual Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=HybridQA:%20A%20Dataset%20of%20Multi-Hop%20Question%20Answering%20over%20Tabular%20and%20Textual%20Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Wenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong, Hong Wang, William Wang</td>
	<td>EMNLP</td>
	<td><p>Existing question answering datasets focus on dealing with homogeneous information, based either only on text or KB/Table information alone. However, as human knowledge is distributed over heterogeneous forms, using homogeneous information alone might lead to severe coverage problems. To fill in the gap, we present HybridQA this https URL, a new large-scale question-answering dataset that requires reasoning on heterogeneous information. Each question is aligned with a Wikipedia table and multiple free-form corpora linked with the entities in the table. The questions are designed to aggregate both tabular information and text information, i.e., lack of either form would render the question unanswerable. We test with three different models: 1) a table-only model. 2) text-only model. 3) a hybrid model that combines heterogeneous information to find the answer. The experimental results show that the EM scores obtained by two baselines are below 20\%, while the hybrid model can achieve an EM over 40\%. This gap suggests the necessity to aggregate heterogeneous information in HybridQA. However, the hybrid model’s score is still far behind human performance. Hence, HybridQA can serve as a challenging benchmark to study question answering with heterogeneous information.</p>
</td>
	<td>sql dataset </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/zhang2019broad/">Broad-Coverage Semantic Parsing as Transduction</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Broad-Coverage Semantic Parsing as Transduction' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Broad-Coverage Semantic Parsing as Transduction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Broad-Coverage%20Semantic%20Parsing%20as%20Transduction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Sheng Zhang, Xutai Ma, Kevin Duh, Benjamin Van Durme</td>
	<td>EMNLP</td>
	<td><p>We unify different broad-coverage semantic parsing tasks into a transduction parsing paradigm, and propose an attention-based neural transducer that incrementally builds meaning representation via a sequence of semantic relations. By leveraging multiple attention mechanisms, the neural transducer can be effectively trained without relying on a pre-trained aligner. Experiments separately conducted on three broad-coverage semantic parsing tasks – AMR, SDP and UCCA – demonstrate that our attention-based neural transducer improves the state of the art on both AMR and UCCA, and is competitive with the state of the art on SDP.</p>
</td>
	<td>amr transformer glove cnn </td>
</tr>



<tr>
	<td>2018</td>
	<td><a href="/publications/liu2018an/">An AMR Aligner Tuned by Transition-based Parser</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=An AMR Aligner Tuned by Transition-based Parser' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=An AMR Aligner Tuned by Transition-based Parser' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=An%20AMR%20Aligner%20Tuned%20by%20Transition-based%20Parser' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Yijia Liu, Wanxiang Che, Bo Zheng, Bing Qin, Ting Liu</td>
	<td>EMNLP</td>
	<td><p>In this paper, we propose a new rich resource enhanced AMR aligner which produces multiple alignments and a new transition system for AMR parsing along with its oracle parser. Our aligner is further tuned by our oracle parser via picking the alignment that leads to the highest-scored achievable AMR graph. Experimental results show that our aligner outperforms the rule-based aligner in previous work by achieving higher alignment F1 score and consistently improving two open-sourced AMR parsers. Based on our aligner and transition system, we develop a transition-based AMR parser that parses a sentence into its AMR graph directly. An ensemble of our parsers with only words and POS tags as input leads to 68.4 Smatch F1 score.</p>
</td>
	<td>amr </td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/lyu2018amr/">AMR Parsing as Graph Prediction with Latent Alignment</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=AMR Parsing as Graph Prediction with Latent Alignment' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=AMR Parsing as Graph Prediction with Latent Alignment' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=AMR%20Parsing%20as%20Graph%20Prediction%20with%20Latent%20Alignment' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Chunchuan Lyu, Ivan Titov</td>
	<td>ACL</td>
	<td><p>Abstract meaning representations (AMRs) are broad-coverage sentence-level semantic representations. AMRs represent sentences as rooted labeled directed acyclic graphs. AMR parsing is challenging partly due to the lack of annotated alignments between nodes in the graphs and words in the corresponding sentences. We introduce a neural parser which treats alignments as latent variables within a joint probabilistic model of concepts, relations and alignments. As exact inference requires marginalizing over alignments and is infeasible, we use the variational autoencoding framework and a continuous relaxation of the discrete alignments. We show that joint modeling is preferable to using a pipeline of align and parse. The parser achieves the best reported results on the standard benchmark (74.4% on LDC2016E25).</p>
</td>
	<td>amr </td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/yu2018spider/">Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Spider:%20A%20Large-Scale%20Human-Labeled%20Dataset%20for%20Complex%20and%20Cross-Domain%20Semantic%20Parsing%20and%20Text-to-SQL%20Task' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, Dragomir Radev</td>
	<td>EMNLP</td>
	<td><p>We present Spider, a large-scale, complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 college students. It consists of 10,181 questions and 5,693 unique complex SQL queries on 200 databases with multiple tables, covering 138 different domains. We define a new complex and cross-domain semantic parsing and text-to-SQL task where different complex SQL queries and databases appear in train and test sets. In this way, the task requires the model to generalize well to both new SQL queries and new database schemas. Spider is distinct from most of the previous semantic parsing tasks because they all use a single database and the exact same programs in the train set and the test set. We experiment with various state-of-the-art models and the best model achieves only 12.4% exact matching accuracy on a database split setting. This shows that Spider presents a strong challenge for future research. Our dataset and task are publicly available at this URL: https://yale-lily.github.io/spider</p>
</td>
	<td>sql cross-domain dataset </td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/dollak2018improving/">Improving Text-to-SQL Evaluation Methodology</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Improving Text-to-SQL Evaluation Methodology' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Improving Text-to-SQL Evaluation Methodology' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Improving%20Text-to-SQL%20Evaluation%20Methodology' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Catherine Finegan-Dollak, Jonathan K. Kummerfeld, Li Zhang, Karthik Ramanathan, Sesh Sadasivam, Rui Zhang, Dragomir Radev</td>
	<td>ACL</td>
	<td><p>To be informative, an evaluation must measure how well systems generalize to realistic unseen data. We identify limitations of and propose improvements to current evaluations of text-to-SQL systems. First, we compare human-generated and automatically generated questions, characterizing properties of queries necessary for real-world applications. To facilitate evaluation on multiple datasets, we release standardized and improved versions of seven existing datasets and one new text-to-SQL dataset. Second, we show that the current division of data into training and test sets measures robustness to variations in the way questions are asked, but only partially tests how well systems generalize to new queries; therefore, we propose a complementary dataset split for evaluation of future work. Finally, we demonstrate how the common practice of anonymizing variables during evaluation removes an important challenge of the task. Our observations highlight key difficulties, and our methodology enables effective measurement of future development.</p>
</td>
	<td>sql dataset </td>
</tr>



<tr>
	<td>2017</td>
	<td><a href="/publications/foland2017abstract/">Abstract Meaning Representation Parsing using LSTM Recurrent Neural Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Abstract Meaning Representation Parsing using LSTM Recurrent Neural Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Abstract Meaning Representation Parsing using LSTM Recurrent Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Abstract%20Meaning%20Representation%20Parsing%20using%20LSTM%20Recurrent%20Neural%20Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>William Foland, James H. Martin</td>
	<td>ACL</td>
	<td><p>We present a system which parses sentences into Abstract Meaning Representations, improving state-of-the-art results for this task by more than 5%. AMR graphs represent semantic content using linguistic properties such as semantic roles, coreference, negation, and more. The AMR parser does not rely on a syntactic pre-parse, or heavily engineered features, and uses five recurrent neural networks as the key architectural components for inferring AMR graphs.</p>
</td>
	<td>amr lstm </td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/wang2017getting/">Getting the Most out of AMR Parsing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Getting the Most out of AMR Parsing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Getting the Most out of AMR Parsing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Getting%20the%20Most%20out%20of%20AMR%20Parsing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Chuan Wang, Nianwen Xue</td>
	<td>EMNLP</td>
	<td><p>This paper proposes to tackle the AMR parsing bottleneck by improving two components of an AMR parser: concept identification and alignment. We first build a Bidirectional LSTM based concept identifier that is able to incorporate richer contextual information to learn sparse AMR concept labels. We then extend an HMM-based word-to-concept alignment model with graph distance distortion and a rescoring method during decoding to incorporate the structural information in the AMR graph. We show integrating the two components into an existing AMR parser results in consistently better performance over the state of the art on various datasets.</p>
</td>
	<td>amr lstm </td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/damonte2017an/">An Incremental Parser for Abstract Meaning Representation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=An Incremental Parser for Abstract Meaning Representation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=An Incremental Parser for Abstract Meaning Representation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=An%20Incremental%20Parser%20for%20Abstract%20Meaning%20Representation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Marco Damonte, Shay B. Cohen, Giorgio Satta</td>
	<td>EACL</td>
	<td><p>Abstract Meaning Representation (AMR) is a semantic representation for natural language that embeds annotations related to traditional tasks such as named entity recognition, semantic role labeling, word sense disambiguation and co-reference resolution. We describe a transition-based parser for AMR that parses sentences left-to-right, in linear time. We further propose a test-suite that assesses specific subtasks that are helpful in comparing AMR parsers, and show that our parser is competitive with the state of the art on the LDC2015E86 dataset and that it outperforms state-of-the-art parsers for recovering named entities and handling polarity.</p>
</td>
	<td>amr </td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/zhong2017seq2sql/">Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Seq2SQL:%20Generating%20Structured%20Queries%20from%20Natural%20Language%20using%20Reinforcement%20Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Victor Zhong, Caiming Xiong, Richard Socher</td>
	<td></td>
	<td><p>A significant amount of the world’s knowledge is stored in relational databases. However, the ability for users to retrieve facts from a database is limited due to a lack of understanding of query languages such as SQL. We propose Seq2SQL, a deep neural network for translating natural language questions to corresponding SQL queries. Our model leverages the structure of SQL queries to significantly reduce the output space of generated queries. Moreover, we use rewards from in-the-loop query execution over the database to learn a policy to generate unordered parts of the query, which we show are less suitable for optimization via cross entropy loss. In addition, we will publish WikiSQL, a dataset of 80654 hand-annotated examples of questions and SQL queries distributed across 24241 tables from Wikipedia. This dataset is required to train our model and is an order of magnitude larger than comparable datasets. By applying policy-based reinforcement learning with a query execution environment to WikiSQL, our model Seq2SQL outperforms attentional sequence to sequence models, improving execution accuracy from 35.9% to 59.4% and logical form accuracy from 23.4% to 48.3%.</p>
</td>
	<td>sql cross-domain dataset reinforcement-learning </td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/konstas2017neural/">Neural AMR: Sequence-to-Sequence Models for Parsing and Generation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Neural AMR: Sequence-to-Sequence Models for Parsing and Generation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Neural AMR: Sequence-to-Sequence Models for Parsing and Generation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Neural%20AMR:%20Sequence-to-Sequence%20Models%20for%20Parsing%20and%20Generation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Ioannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin Choi, Luke Zettlemoyer</td>
	<td>ACL</td>
	<td><p>Sequence-to-sequence models have shown strong performance across a broad range of applications. However, their application to parsing and generating text usingAbstract Meaning Representation (AMR)has been limited, due to the relatively limited amount of labeled data and the non-sequential nature of the AMR graphs. We present a novel training procedure that can lift this limitation using millions of unlabeled sentences and careful preprocessing of the AMR graphs. For AMR parsing, our model achieves competitive results of 62.1SMATCH, the current best score reported without significant use of external semantic resources. For AMR generation, our model establishes a new state-of-the-art performance of BLEU 33.8. We present extensive ablative and qualitative analysis including strong evidence that sequence-based AMR models are robust against ordering variations of graph-to-sequence conversions.</p>
</td>
	<td>amr </td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/ballesteros2017amr/">AMR Parsing using Stack-LSTMs</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=AMR Parsing using Stack-LSTMs' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=AMR Parsing using Stack-LSTMs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=AMR%20Parsing%20using%20Stack-LSTMs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Miguel Ballesteros, Yaser Al-Onaizan</td>
	<td>EMNLP</td>
	<td><p>We present a transition-based AMR parser that directly generates AMR parses from plain text. We use Stack-LSTMs to represent our parser state and make decisions greedily. In our experiments, we show that our parser achieves very competitive scores on English using only AMR training data. Adding additional information, such as POS tags and dependency trees, improves the results further.</p>
</td>
	<td>amr lstm </td>
</tr>



<tr>
	<td>2016</td>
	<td><a href="/publications/zhou2016amr/">AMR Parsing with an Incremental Joint Model</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=AMR Parsing with an Incremental Joint Model' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=AMR Parsing with an Incremental Joint Model' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=AMR%20Parsing%20with%20an%20Incremental%20Joint%20Model' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Junsheng Zhou, Feiyu Xu, Hans Uszkoreit, Weiguang Qu, Ran Li, Yanhui Gu</td>
	<td>EMNLP</td>
	<td><p>To alleviate the error propagation in the traditional pipelined models for Abstract Meaning Representation (AMR) parsing, we formulate AMR parsing as a joint task that performs the two subtasks: concept identification and relation identification simultaneously. To this end, we first develop a novel componentwise beam search algorithm for relation identification in an incremental fashion, and then incorporate the decoder into a unified framework based on multiple-beam search, which allows for the bi-directional information flow between the two subtasks in a single incremental model. Experiments on the public datasets demonstrate that our joint model significantly outperforms the previous pipelined counterparts, and also achieves better or comparable performance than other approaches to AMR parsing, without utilizing external semantic resources.</p>
</td>
	<td>amr </td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/wang2016camr/">CAMR at SemEval-2016 Task 8: An Extended Transition-based AMR Parser</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=CAMR at SemEval-2016 Task 8: An Extended Transition-based AMR Parser' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=CAMR at SemEval-2016 Task 8: An Extended Transition-based AMR Parser' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=CAMR%20at%20SemEval-2016%20Task%208:%20An%20Extended%20Transition-based%20AMR%20Parser' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Chuan Wang, Sameer Pradhan, Xiaoman Pan, Heng Ji, Nianwen Xue</td>
	<td>SemEval</td>
	<td><p>This paper describes CAMR, the transitionbased parser that we use in the SemEval-2016 Meaning Representation Parsing task. The main contribution of this paper is a description of the additional sources of information that we use as features in the parsing model to further boost its performance. We start with our existing AMR parser and experiment with three sets of new features: 1) rich named entities, 2) a verbalization list, 3) semantic role labels. We also use the RPI Wikifier to wikify the concepts in the AMR graph. Our parser achieves a Smatch F-score of 62% on the official blind test set.</p>
</td>
	<td>amr </td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/goodman2016noise/">Noise reduction and targeted exploration in imitation learning for Abstract Meaning Representation parsing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Noise reduction and targeted exploration in imitation learning for Abstract Meaning Representation parsing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Noise reduction and targeted exploration in imitation learning for Abstract Meaning Representation parsing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Noise%20reduction%20and%20targeted%20exploration%20in%20imitation%20learning%20for%20Abstract%20Meaning%20Representation%20parsing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>James Goodman, Andreas Vlachos, Jason Naradowsky</td>
	<td>ACL</td>
	<td><p>Semantic parsers map natural language statements into meaning representations, and must abstract over syntactic phenomena, resolve anaphora, and identify word senses to eliminate ambiguous interpretations. Abstract meaning representation (AMR) is a recent example of one such semantic formalism which, similar to a dependency parse, utilizes a graph to represent relationships between concepts (Banarescu et al., 2013). As with dependency parsing, transition-based approaches are a common approach to this problem. However, when trained in the traditional manner these systems are susceptible to the accumulation of errors when they find undesirable states during greedy decoding. Imitation learning algorithms have been shown to help these systems recover from such errors. To effectively use these methods for AMR parsing we find it highly beneficial to introduce two novel extensions: noise reduction and targeted exploration. The former mitigates the noise in the feature representation, a result of the complexity of the task. The latter targets the exploration steps of imitation learning towards areas which are likely to provide the most information in the context of a large action-space. We achieve state-ofthe art results, and improve upon standard transition-based parsing by 4.7 F1 points.</p>
</td>
	<td>amr </td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/flanigan2016cmu/">CMU at SemEval-2016 Task 8: Graph-based AMR Parsing with Infinite Ramp Loss</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=CMU at SemEval-2016 Task 8: Graph-based AMR Parsing with Infinite Ramp Loss' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=CMU at SemEval-2016 Task 8: Graph-based AMR Parsing with Infinite Ramp Loss' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=CMU%20at%20SemEval-2016%20Task%208:%20Graph-based%20AMR%20Parsing%20with%20Infinite%20Ramp%20Loss' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Jeffrey Flanigan, Chris Dyer, Noah A. Smith, Jaime Carbonell</td>
	<td>SemEval</td>
	<td><p>We present improvements to the JAMR parser as part of the SemEval 2016 Shared Task 8 on AMR parsing. The major contributions are: improved concept coverage using external resources and features, an improved aligner, and a novel loss function for structured prediction called infinite ramp, which is a generalization of the structured SVM to problems with unreachable training instances.</p>
</td>
	<td>amr svm </td>
</tr>



<tr>
	<td>2015</td>
	<td><a href="/publications/wang2015boosting/">Boosting Transition-based AMR Parsing with Refined Actions and Auxiliary Analyzers</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Boosting Transition-based AMR Parsing with Refined Actions and Auxiliary Analyzers' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Boosting Transition-based AMR Parsing with Refined Actions and Auxiliary Analyzers' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Boosting%20Transition-based%20AMR%20Parsing%20with%20Refined%20Actions%20and%20Auxiliary%20Analyzers' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Chuan Wang, Nianwen Xue, Sameer Pradhan</td>
	<td>ACL</td>
	<td><p>We report improved AMR parsing results by adding a new action to a transitionbased AMR parser to infer abstract concepts and by incorporating richer features produced by auxiliary analyzers such as a semantic role labeler and a coreference resolver. We report final AMR parsing results that show an improvement of 7% absolute in F1 score over the best previously reported result. Our parser is available at: https://github.com/Juicechuan/AMRParsing</p>
</td>
	<td>amr </td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/pust2015parsing/">Parsing English into Abstract Meaning Representation Using Syntax-Based Machine Translation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Parsing English into Abstract Meaning Representation Using Syntax-Based Machine Translation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Parsing English into Abstract Meaning Representation Using Syntax-Based Machine Translation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Parsing%20English%20into%20Abstract%20Meaning%20Representation%20Using%20Syntax-Based%20Machine%20Translation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Michael Pust, Ulf Hermjakob, Kevin Knight, Daniel Marcu, Jonathan May</td>
	<td>EMNLP</td>
	<td><p>We present a parser for Abstract Meaning Representation (AMR). We treat Englishto-AMR conversion within the framework of string-to-tree, syntax-based machine translation (SBMT). To make this work, we transform the AMR structure into a form suitable for the mechanics of SBMT and useful for modeling. We introduce an AMR-specific language model and add data and features drawn from semantic resources. Our resulting AMR parser significantly improves upon state-of-the-art results.</p>
</td>
	<td>amr </td>
</tr>



<tr>
	<td>2013</td>
	<td><a href="/publications/banarescu2013abstract/">Abstract Meaning Representation for Sembanking</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Abstract Meaning Representation for Sembanking' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Abstract Meaning Representation for Sembanking' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Abstract%20Meaning%20Representation%20for%20Sembanking' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, Nathan Schneider</td>
	<td>LAW</td>
	<td><p>We describe Abstract Meaning Representation (AMR), a semantic representation language in which we are writing down the meanings of thousands of English sentences. We hope that a sembank of simple, whole-sentence semantic structures will spur new work in statistical natural language understanding and generation, like the Penn Treebank encouraged work on statistical parsing. This paper gives an overview of AMR and tools associated with it.</p>
</td>
	<td>amr dataset </td>
</tr>


</tbody></table>

<script>
var datatable;

function searchTable() {
    var hash = decodeURIComponent(window.location.hash.substr(1));
    datatable.search(hash).draw();
}


$(document).ready( function () {
    datatable = $('#allPapers').DataTable({
		paging: false,
		"order": [[ 0, 'desc' ], [ 1, 'asc' ]],
		columnDefs: [
			{
				targets: [3, 4, 5],
				visible: false,
				searchable: true
			}]
		});
    searchTable();
});

$(window).on('hashchange', function() {
  searchTable();
});
</script>


    </div>

  </body>
</html>
